{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32c08ee7-fd95-4b6e-8c31-4214ef9251c0",
   "metadata": {},
   "source": [
    "# ChatGPT fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2875686f-f3cc-4407-beb4-c685bc5899e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e61487-2f34-4dc6-8925-4b8edd0a6e8d",
   "metadata": {},
   "source": [
    "## Create training data file from logged messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "641ec975-cccf-4f2e-8946-0ee7ef155037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft_examples/messages_129.json\n",
      "ft_examples/messages_105.json\n",
      "ft_examples/messages_113.json\n",
      "ft_examples/messages_125.json\n",
      "ft_examples/messages_46.json\n",
      "ft_examples/messages_109.json\n",
      "ft_examples/messages_108.json\n",
      "ft_examples/messages_47.json\n",
      "ft_examples/messages_124.json\n",
      "ft_examples/messages_112.json\n",
      "ft_examples/messages_104.json\n",
      "ft_examples/messages_128.json\n",
      "ft_examples/messages_123.json\n",
      "ft_examples/messages_40.json\n",
      "ft_examples/messages_119.json\n",
      "ft_examples/messages_103.json\n",
      "ft_examples/messages_115.json\n",
      "ft_examples/messages_114.json\n",
      "ft_examples/messages_102.json\n",
      "ft_examples/messages_118.json\n",
      "ft_examples/messages_41.json\n",
      "ft_examples/messages_122.json\n",
      "ft_examples/messages_42.json\n",
      "ft_examples/messages_121.json\n",
      "ft_examples/messages_117.json\n",
      "ft_examples/messages_101.json\n",
      "ft_examples/messages_100.json\n",
      "ft_examples/messages_116.json\n",
      "ft_examples/messages_120.json\n",
      "ft_examples/messages_43.json\n",
      "ft_examples/messages_111.json\n",
      "ft_examples/messages_48.json\n",
      "ft_examples/messages_107.json\n",
      "ft_examples/messages_44.json\n",
      "ft_examples/messages_127.json\n",
      "ft_examples/messages_126.json\n",
      "ft_examples/messages_45.json\n",
      "ft_examples/messages_106.json\n",
      "ft_examples/messages_49.json\n",
      "ft_examples/messages_110.json\n",
      "Num examples: 40\n"
     ]
    }
   ],
   "source": [
    "input_data_path = \"ft_examples\"\n",
    "output_data_path = \"ft_examples/training_file_with_weights.jsonl\"\n",
    "\n",
    "input_files = [os.path.join(input_data_path, filename) for filename in os.listdir(input_data_path) if \"messages\" in filename]\n",
    "\n",
    "# Load the dataset\n",
    "dataset = []\n",
    "for filename in input_files:\n",
    "    print(filename)\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        dataset.append(json.load(f))\n",
    "\n",
    "print(\"Num examples:\", len(dataset))\n",
    "\n",
    "# Fix format\n",
    "for ex in dataset:\n",
    "    messages = ex.get(\"messages\", None)\n",
    "    last_message = {\"role\" : \"\", \"content\" : \"\"}\n",
    "    for message in messages:\n",
    "        # remove arguments from functions (they are not allowed in GPT prompt)\n",
    "        if \"arguments\" in message:\n",
    "            del message[\"arguments\"] \n",
    "        # set weights to 1 only for messages interpreting function responses (typically offering products from vector DB)\n",
    "        if last_message[\"role\"] == \"function\" and message[\"role\"] == \"assistant\":\n",
    "            message[\"weight\"] = 1\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            message[\"weight\"] = 0\n",
    "        last_message = message\n",
    "    functions = ex.get(\"functions\", None)\n",
    "    for fc in functions:\n",
    "        if fc[\"parameters\"] is None:\n",
    "            fc[\"parameters\"] = {}\n",
    "data = {}\n",
    "with open(output_data_path, \"w\", encoding = \"UTF-8\") as f:\n",
    "    for ex in dataset:\n",
    "        json.dump(ex, f, ensure_ascii=False) \n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8918dc6-9fe1-478b-90a4-84877c60e526",
   "metadata": {},
   "source": [
    "## Format validation\n",
    "\n",
    "We can perform a variety of error checks to validate that each conversation in the dataset adheres to the format expected by the fine-tuning API. Errors are categorized based on their nature for easier debugging.\n",
    "\n",
    "1. **Data Type Check**: Checks whether each entry in the dataset is a dictionary (`dict`). Error type: `data_type`.\n",
    "2. **Presence of Message List**: Checks if a `messages` list is present in each entry. Error type: `missing_messages_list`.\n",
    "3. **Message Keys Check**: Validates that each message in the `messages` list contains the keys `role` and `content`. Error type: `message_missing_key`.\n",
    "4. **Unrecognized Keys in Messages**: Logs if a message has keys other than `role`, `content`, `weight`, `function_call`, and `name`. Error type: `message_unrecognized_key`.\n",
    "5. **Role Validation**: Ensures the `role` is one of \"system\", \"user\", or \"assistant\". Error type: `unrecognized_role`.\n",
    "6. **Content Validation**: Verifies that `content` has textual data and is a string. Error type: `missing_content`.\n",
    "7. **Assistant Message Presence**: Checks that each conversation has at least one message from the assistant. Error type: `example_missing_assistant_message`.\n",
    "\n",
    "The code below performs these checks, and outputs counts for each type of error found are printed. This is useful for debugging and ensuring the dataset is ready for the next steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cd32772-bdc0-49dd-af84-fd652520f63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 40\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a Bakery Salesman. Help user buy bakery goods. Introduce yourself and ask user how you can help. Komunikuj v češtině.'}\n",
      "{'role': 'user', 'content': 'Dobrý den'}\n",
      "{'role': 'assistant', 'content': 'Dobrý den, vítejte! Jak vám mohu pomoci dnes ohledně našich pečivových výrobků? Chcete si něco vybrat nebo potřebujete poradit s výběrem?', 'weight': 0}\n",
      "{'role': 'user', 'content': 'Dobrý den, rád bych 3 kousky klasických makových koláčků, prosím. Děkuji.'}\n",
      "{'role': 'function', 'name': 'add_item_to_cart', 'content': \"Wrong item name. We can offer these similar items: ['Špička s náplní makovou 2ks á 110g', 'Koláč šáteček 75g', 'Špička s náplní makovou a tvarohovou 2ks á110g', 'Vánočka mandlová 400g']. \"}\n",
      "{'role': 'assistant', 'content': 'Omlouvám se, ale nemáme přesně \"klasické makové koláčky\". Máme však několik podobných možností, například \"Špička s náplní makovou 2ks á 110g\", \"Koláč šáteček 75g\" nebo \"Špička s náplní makovou a tvarohovou 2ks á110g\". Mohu vám s něčím z těchto možností pomoci nebo se rozhodnete pro něco jiného? Děkuji.', 'weight': 1}\n",
      "{'role': 'user', 'content': 'Ano, rád si dám \"Špičku s náplní makovou\". Děkuji.'}\n",
      "{'role': 'function', 'name': 'add_item_to_cart', 'content': 'Item added to the cart'}\n",
      "{'role': 'assistant', 'content': 'Skvělá volba! \"Špička s náplní makovou 2ks á 110g\" byla úspěšně přidána do vašeho košíku. Pokud to bude vše, můžeme pokračovat k platbě nebo ještě něco potřebujete? Děkuji.', 'weight': 1}\n",
      "{'role': 'user', 'content': 'Ano, to by bylo vše. Můžeme pokračovat k platbě. Děkuji za vaši pomoc.'}\n",
      "{'role': 'function', 'name': 'checkout', 'content': 'Done'}\n",
      "{'role': 'assistant', 'content': 'Vaše objednávka byla úspěšně zpracována. Děkuji vám za nákup a přeji pěkný den! Pokud budete chtít nakupovat zase, neváhejte se na nás obrátit. Hezký den!', 'weight': 1}\n",
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "data_path = output_data_path\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "  #  dataset = json.load(f)\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)\n",
    "    \n",
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "            \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            print(k)\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb03447-fdd2-4be5-8a79-610539080524",
   "metadata": {},
   "source": [
    "## Data Warnings and Token Counts \n",
    "\n",
    "With some lightweight analysis we can identify potential issues in the dataset, like missing messages, and provide statistical insights into message and token counts.\n",
    "\n",
    "1. **Missing System/User Messages**: Counts the number of conversations missing a \"system\" or \"user\" message. Such messages are critical for defining the assistant's behavior and initiating the conversation.\n",
    "2. **Number of Messages Per Example**: Summarizes the distribution of the number of messages in each conversation, providing insight into dialogue complexity.\n",
    "3. **Total Tokens Per Example**: Calculates and summarizes the distribution of the total number of tokens in each conversation. Important for understanding fine-tuning costs.\n",
    "4. **Tokens in Assistant's Messages**: Calculates the number of tokens in the assistant's messages per conversation and summarizes this distribution. Useful for understanding the assistant's verbosity.\n",
    "5. **Token Limit Warnings**: Checks if any examples exceed the maximum token limit (4096 tokens), as such examples will be truncated during fine-tuning, potentially resulting in data loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c570d69-bef4-481a-8475-d2fe69af9b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            if key != \"weight\":\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bcc0ad9-c18c-4f52-a944-d6755f62d9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 10, 29\n",
      "mean / median: 14.3, 14.0\n",
      "p5 / p95: 12.0, 18.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 418, 1280\n",
      "mean / median: 661.95, 643.0\n",
      "p5 / p95: 444.2, 874.0000000000002\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 208, 703\n",
      "mean / median: 364.05, 347.0\n",
      "p5 / p95: 240.8, 468.6000000000001\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26237674-5ad5-4d81-bd68-9459a856cef5",
   "metadata": {},
   "source": [
    "## Cost Estimation\n",
    "\n",
    "In this final section, we estimate the total number of tokens that will be used for fine-tuning, which allows us to approximate the cost. It is worth noting that the duration of the fine-tuning jobs will also increase with the token count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39db57da-e384-453e-8cd5-b609995f94bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~26478 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~79434 tokens\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd53d24d-98dd-4fc4-a32e-0c26142aff5d",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ccdfb6-b492-4c83-a88c-5653a6072347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload training file\n",
    "client = OpenAI()\n",
    "\n",
    "fo = client.files.create(\n",
    "  file=open(\"training_file_with_weights.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43123f3-7356-49bd-a5d0-bfa73c397ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fine-tuning job\n",
    "client.fine_tuning.jobs.create(\n",
    "  training_file=fo.id, \n",
    "  model=\"gpt-3.5-turbo-0125\",\n",
    "  suffix=\"baker_weights_e2\",\n",
    "  hyperparameters = {\n",
    "      \"n_epochs\": 2,\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c5105a-3138-4f10-973f-cc5fc29b1081",
   "metadata": {},
   "source": [
    "## List of fine-tuned models\n",
    "\n",
    "    - \"ft:gpt-3.5-turbo-0125:born-digital-s-r-o:baker-weights-e2:976VKvlB\"\n",
    "        - train file \"ft_examples/training_file_with_weights.jsonl\"\n",
    "        - 2 epochs\n",
    "        - fine-tuned only on respomses following function calls (typically offering products from vector DB)\n",
    "        \n",
    "    - \"ft:gpt-3.5-turbo-0125:born-digital-s-r-o:baker2-e2:96vWpkfj\",\n",
    "        - train file \"ft_examples/training_file.jsonl\n",
    "        - 2 epochs\n",
    "        - fine-tuned on all assistant responses\n",
    "        - does not work well (especially function calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328e4304-bd6c-487f-bb1f-b6a8f4717519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
